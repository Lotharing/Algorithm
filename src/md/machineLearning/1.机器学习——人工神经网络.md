# 1.机器学习——人工神经网络

### 感知器（Perceptron）

由于svm过于复杂，所以先从容易的感知器开始

---

#### 神经元模型

<img src=".\res\1.1神经元模型.jpg" style="zoom:50%;" />

<img src=".\res\1.2神经元数学模型.png" style="zoom:50%;" />

第二幅图中最后的等式中的$W^T_k$ 是向量W的转置，X就是向量X，所以W的转置乘X等于等式左边的求和





#### 感知器算法

<img src=".\res\1.3感知器算法.png" style="zoom: 25%;" />

上图的W,X大写指的是向量(不仅仅是一维，也包括高维度)

根据上图中i，当$W^T$X+b > 0, y = -1是不满足区分条件的，此时的W和b不能正确分类，所以新W = W - X, 新b = b - 1   (1)

则此时$W^T$X + b = 新$W^T$X + 新b （2）

将(1)代入到(2)有$(W - X)^T$X + b - 1  = ($W^T$X +b) - ($||X||^2$ + 1)                   *  X$X^T$ = X的行列式的平方

所以每进行一次(1) 就会使方程减小($||X||^2$ + 1)  这样只需要一直进行下去就会使(i)不满足，(ii)同理

*如何证明只需要有限次的(i)操作，就能终止?

​	定义一个增广向量$$\overrightarrow{x}$$，a.若y = +1,则x = $[X, 1]^T$

​											b.若y = -1,则x = $[-X, -1]^T$

​	定义一个增广向量$\overrightarrow{w}$， w= $[W, b]^T$                                                             * 为方便用w替代增广向量

​	上图感知器算法中(i)和(ii)就变成了   若$w^T$x < 0，则w = w + x                  *  w的转置乘以向量x恰好等于W^TX + b

​	真正的证明开始了

​	证: 不失一般性，设$||w||$ = 1                                                                      *  因为w与aw使同一个平面，所以通过改变a让w为1

​		假设第k步的w是$w_k$，且有一个$x_i$使$w^T_k$$x_i$ < 0

​		则根据感知器算法 $w_{(k+1)}$ = $w_k$ + $x_i$

​		$w_{(k+1)}$ - a$w_i$ = $w_k$ + $x_i$ - a$w_i$

​		$|| w_{(k+1)} - aw_i  ||^2$ =  $|| w_k + x_i - aw_i ||^2$

​										= $|| w_k - aw_i ||^2$ + $||x_i||^2$ + 2$w^T_kx_i$ - 2a$w_i^Tx_i$

​		一定可以取很大的一个a，使$|| w_{(k+1)} - aw_i  ||^2 < || w_k - aw_i ||^2$ ，所以随k增加在减小

​		后面还有定量分析，算了...

##### 感知器算法的收敛定理

输入$x_i$ (i属于1~N)，若线性可分，即存在$w_i$使 $w_i^Tx_i$ > 0，则根据感知器算法，经过优先步得到一个w，使$w^Tx_i$  > 0



