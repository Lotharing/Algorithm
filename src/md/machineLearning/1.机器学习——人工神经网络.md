# 1.机器学习——人工神经网络

### 感知器（Perceptron）

由于svm过于复杂，所以先从容易的感知器开始

---

#### 神经元模型

<img src=".\res\1.1神经元模型.jpg" style="zoom:50%;" />

<img src=".\res\1.2神经元数学模型.png" style="zoom:50%;" />

第二幅图中最后的等式中的(W^Tk)是向量W的转置，X就是向量X，所以W的转置乘X等于等式左边的求和





#### 感知器算法

<img src=".\res\1.3感知器算法.png" style="zoom: 25%;" />

上图的W,X大写指的是向量(不仅仅是一维，也包括高维度)

根据上图中i，当W^TX+b > 0, y = -1是不满足区分条件的，此时的W和b不能正确分类，所以新W = W - X, 新b = b - 1   (1)

则此时WtX + b = 新WtX + 新b （2）

将(1)代入到(2)有(W - X)^TX + b - 1  = (W^TX +b) - (||X||^2 + 1)                        *  XX^T = X的行列式的平方

所以每进行一次(1) 就会使方程减小(||X||^2 + 1)  这样只需要一直进行下去就会使(i)不满足，(ii)同理

*如何证明只需要有限次的(i)操作，就能终止?

​	定义一个增广向量x，a.若y = +1,则x = [X, 1]^T

​											b.若y = -1,则x = [-X, -1]^T

​	定义一个增广向量w，w = [W, b]^T

​	上图感知器算法中(i)和(ii)就变成了   若w^Tx < 0，则w = w + x                  *  w的转置乘以向量x恰好等于W^TX + b

​	真正的证明开始了

​	证: 不失一般性，设||w|| = 1                                                                       *  因为w与aw使同一个平面，所以通过改变a让w为1

​		假设第k步的w是wk，且有一个xi使w^Tkxi < 0

​		则根据感知器算法 w(k+1) = wk + xi

​		w(k+1) - awi = wk + xi - awi

​		|| w(k+1) - awi  ||^2 =  || wk + xi - awi ||^2

​										= || wk - awi ||^2 + ||xi||^2 + 2w^Tkxi - 2awi^Txi

​		一定可以取很大的一个a，使|| w(k+1) - awi  ||^2 < || wk - awi ||^2 ，所以随k增加在减小

​		后面还有定量分析，算了...

##### 感知器算法的收敛定理

输入xi (i属于1~N)，若线性可分，即存在wi使 wi^Txi > 0，则根据感知器算法，经过优先步得到一个w，使w^Txi  > 0

